version: '3.8'
services:

  scheduler:
    image: top-1m-jarm:latest
    command: poetry run python /code/top1Mjarm/scheduler.py
    depends_on:
      - redis_queue
      - worker
    environment:
      REDIS_HOST: redis_queue
      REDIS_PASSWORD: XXX_SET_REDIS_PASS_XXX
    volumes:
#      - path/to/top-1m.csv:/code/top-1m.csv
      - ./top-1m.csv:/code/top-1m.csv
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3
        delay: 30s
      resources:
        limits:
          cpus: '1'
          memory: 100M

  redis_queue:
    image: redis:7-alpine
    command: redis-server --requirepass XXX_SET_REDIS_PASS_XXX
    volumes:
      - ./data/redis_data:/data
    deploy:
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '1'
          memory: 50M

  worker:
    image: top-1m-jarm:latest
    command: poetry run rq worker default domains ips --with-scheduler --url redis://:XXX_SET_REDIS_PASS_XXX@redis_queue:6379 # --sentry-dsn XXX_SET_SENTRY_DSN_XXX
    depends_on:
      - redis_queue
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3
        delay: 30s
      resources:
        limits:
          cpus: '0.5'
          memory: 100M
      update_config:
        parallelism: 0  # Restart all at the same time
      mode: replicated
      replicas: 4

  csv_writer:  # Special worker that only take care of writing the csv result file
    image: top-1m-jarm:latest
    command: poetry run rq worker jarm_result --with-scheduler --url redis://:XXX_SET_REDIS_PASS_XXX@redis_queue:6379 # --sentry-dsn XXX_SET_SENTRY_DSN_XXX
    depends_on:
      - redis_queue
    volumes:
#      - path/to/result.csv:/code/result.csv
      - ./result.csv:/code/result.csv
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3
        delay: 30s
      resources:
        limits:
          cpus: '1'
          memory: 100M
      mode: replicated
      replicas: 1